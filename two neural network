import pandas as pd
from sklearn.preprocessing import OneHotEncoder
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.utils import to_categorical
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss, classification_report
import matplotlib.pyplot as plt
df = pd.read_excel(r'D:\Career\wrtiting paper\first paper\FGAP\neural network\New folder\data.xlsx')
co = pd.read_excel(r'C:\Users\HessaM\Master\coefficient.xlsx')
va = pd.read_excel(r'C:\Users\HessaM\Master\variables.xlsx')
second_row = df.iloc[0,3:]
second_row_list = second_row.tolist()
first_row = []
for i in second_row_list:
    first_row.append(i)
column1 = df.loc[1:298 ,'Unnamed: 2']
column1_list = column1.tolist()
first_column = []
for i in column1_list:
    first_column.append(i)
input_list = []
num_vars = 298
for i in range(1, num_vars+1):
    var_name = f"input_{i}"
    exec(f"{var_name} = []")
    input_list.append(eval(var_name))
co_list = []
num_vars = 298
for i in range(1, num_vars+1):
    var_name = f"co_{i}"
    exec(f"{var_name} = []")
    co_list.append(eval(var_name))
va_list = []
num_vars = 298
for i in range(1, num_vars+1):
    var_name = f"va_{i}"
    exec(f"{var_name} = []")
    va_list.append(eval(var_name))
indx = 0
col_indx = 0
input_indx = 0
while indx < 298:
  for i in first_row:
    input_list[input_indx].append(first_column[col_indx]+','+i)
  col_indx += 1
  input_indx += 1
  indx += 1


    
new_input_data = []

for sublist in input_list:
    new_sublist = []
    for i, element in enumerate(sublist):
        new_sublist.append([float(x) for x in element.split(',')])
    new_input_data.append(new_sublist)
third_row_co = co.iloc[1,3:]
third_row_list_co = third_row_co.tolist()
third_row_co = []
for i in third_row_list_co:
    third_row_co.append(i)
third_row_co[4]
'1'
third_row_va = va.iloc[1,3:]
third_row_list_va = third_row_va.tolist()
third_row_va = []
for i in third_row_list_va:
    third_row_va.append(i)
third_row_va[4]
'landa2'
row_indx = 1
indx = 0
while indx < 298:
    row = co.iloc[row_indx,3:]
    row_lst = row.tolist()
    for i in row_lst:
        co_list[indx].append(i)
    row_indx += 1
    indx += 1
    
row_indx = 1
indx = 0
while indx < 298:
    row = va.iloc[row_indx,3:]
    row_lst = row.tolist()
    for i in row_lst:
        va_list[indx].append(i)
    row_indx += 1
    indx += 1
    
def one_hot_encode(sublist_list):
    encoded_data = []
    feature_names = []
    
    unique_elements = set([item for sublist in sublist_list for item in sublist])
    encoding_dict = {element: i for i, element in enumerate(unique_elements)}
    
    for sublist in sublist_list:
        encoded_sublist = []
        for element in sublist:
            encoding = [0] * len(unique_elements)
            encoding[encoding_dict[element]] = 1
            encoded_sublist.append(encoding)
        encoded_data.append(encoded_sublist)
    for element in unique_elements:
        feature_names.append(element)
    return encoded_data, feature_names
co_encoded_list, co_feature_list = one_hot_encode(co_list)
va_encoded_list, va_feature_list = one_hot_encode(va_list)
co_feature_list
[0, '2', '3+1+3', '1+1+1', '1+1+1+1', '3', '1', '1+3+3', '3+3', '1+1']
va_feature_list
[0,
 "landa'1",
 "landa'1+landa1",
 "landa1+landa'1",
 'landa2',
 'mu3+mu4+mu5',
 'mu4',
 "landa1+landa'2",
 'mu3',
 'mu1',
 'mu5',
 'mu4+mu5',
 "landa1+landa'1+landa'2",
 "landa1+landa'1+landa2",
 "landa1+landa2+landa'2",
 "landa2+landa'2",
 'mu2',
 'landa1+landa2',
 "landa'2+landa2",
 "landa'2",
 "landa1+landa'1+landa2+landa'2",
 'mu1+mu2+mu5',
 'landa1']
print(len(va_feature_list), len(co_feature_list))
23 10
input_data = np.reshape(new_input_data , (298*298, 10))
output_data = np.reshape(co_encoded_list , (298*298, 10))

model_co = Sequential()
model_co.add(Dense(64, input_shape=(10,), activation='relu'))
model_co.add(Dense(10, activation='softmax'))

model_co.compile(loss='categorical_crossentropy', optimizer='adam')


model_co.fit(input_data, output_data , epochs=40)
Epoch 1/40
2776/2776 [==============================] - 6s 2ms/step - loss: 0.1291
Epoch 2/40
2776/2776 [==============================] - 6s 2ms/step - loss: 0.0657
Epoch 3/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0525
Epoch 4/40
2776/2776 [==============================] - 6s 2ms/step - loss: 0.0444
Epoch 5/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0398
Epoch 6/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0367
Epoch 7/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0344
Epoch 8/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0327
Epoch 9/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0307
Epoch 10/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0289
Epoch 11/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0271
Epoch 12/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0251
Epoch 13/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0233
Epoch 14/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0212
Epoch 15/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0197
Epoch 16/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0182
Epoch 17/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0165
Epoch 18/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0158
Epoch 19/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0145
Epoch 20/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0136
Epoch 21/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0130
Epoch 22/40
2776/2776 [==============================] - 6s 2ms/step - loss: 0.0123
Epoch 23/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0114
Epoch 24/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0111
Epoch 25/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0104
Epoch 26/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0103
Epoch 27/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0098
Epoch 28/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0096
Epoch 29/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0089
Epoch 30/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0088
Epoch 31/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0085
Epoch 32/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0082
Epoch 33/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0080
Epoch 34/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0078
Epoch 35/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0077
Epoch 36/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0074
Epoch 37/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0074
Epoch 38/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0071
Epoch 39/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0068
Epoch 40/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0067
<keras.callbacks.History at 0x267847366d0>
input_data = np.reshape(new_input_data , (298*298, 10))
output_data = np.reshape(va_encoded_list , (298*298, 23))

model_va = Sequential()
model_va.add(Dense(64, input_shape=(10,), activation='relu'))
model_va.add(Dense(23, activation='softmax'))

model_va.compile(loss='categorical_crossentropy', optimizer='adam')


model_va.fit(input_data, output_data , epochs=40)
Epoch 1/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.1642
Epoch 2/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0676
Epoch 3/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0396
Epoch 4/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0268
Epoch 5/40
2776/2776 [==============================] - 4s 2ms/step - loss: 0.0204
Epoch 6/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0168
Epoch 7/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0148
Epoch 8/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0129
Epoch 9/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0117
Epoch 10/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0106
Epoch 11/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0097
Epoch 12/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0089
Epoch 13/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0082
Epoch 14/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0078
Epoch 15/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0071
Epoch 16/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0066
Epoch 17/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0064
Epoch 18/40
2776/2776 [==============================] - 4s 2ms/step - loss: 0.0060
Epoch 19/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0056
Epoch 20/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0054
Epoch 21/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0051
Epoch 22/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0047
Epoch 23/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0047
Epoch 24/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0046
Epoch 25/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0043
Epoch 26/40
2776/2776 [==============================] - 4s 2ms/step - loss: 0.0042
Epoch 27/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0041
Epoch 28/40
2776/2776 [==============================] - 4s 2ms/step - loss: 0.0039
Epoch 29/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0037
Epoch 30/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0038
Epoch 31/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0034
Epoch 32/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0034
Epoch 33/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0032
Epoch 34/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0033
Epoch 35/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0031
Epoch 36/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0032
Epoch 37/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0030
Epoch 38/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0029
Epoch 39/40
2776/2776 [==============================] - 5s 2ms/step - loss: 0.0029
Epoch 40/40
2776/2776 [==============================] - 4s 2ms/step - loss: 0.0029
<keras.callbacks.History at 0x267848479a0>
input_data = np.array([-3,-1,-1,-3,-3,-3,-1,-1,-4,-3])
print('Start:',input_data[0:5])
print('End:',' ', input_data[5:10])
print('\nWhole 10 array:',input_data, "\n")
input_data = np.reshape(input_data, (1, 10))
print('PREDICTION IS BELOW:')
predictions_co = model_co.predict(input_data)
predictions_va = model_va.predict(input_data)

print(predictions_co, "\n", predictions_va )

predicted_class_co = model_co.predict(input_data.reshape(1, 10))
predicted_class_va = model_va.predict(input_data.reshape(1, 10))

predicted_class_index_co = predicted_class_co.argmax()
predicted_class_index_va = predicted_class_va.argmax()

featurss_co = co_feature_list[predicted_class_index_co]
featurss_va = va_feature_list[predicted_class_index_va]

print("Predicted class index of coefficient:", predicted_class_index_co,'\nThe label of class index of coefficient:', featurss_co,'\nPredicted class index of variable:',predicted_class_index_va,'\nThe label of class index of variable:', featurss_va)
print("\n------\n")
print('The final answer:', featurss_co+featurss_va)
Start: [-3 -1 -1 -3 -3]
End:   [-3 -1 -1 -4 -3]

Whole 10 array: [-3 -1 -1 -3 -3 -3 -1 -1 -4 -3] 

PREDICTION IS BELOW:
1/1 [==============================] - 0s 31ms/step
1/1 [==============================] - 0s 22ms/step
[[5.2672621e-02 8.1118114e-06 7.1621712e-25 1.5136477e-25 8.4661506e-33
  1.2131358e-07 9.4731885e-01 1.1599233e-21 1.6246322e-17 1.9779858e-07]] 
 [[3.0100151e-04 1.1910705e-19 0.0000000e+00 2.0004550e-23 1.4137555e-10
  5.3090196e-30 9.9969900e-01 2.3674951e-29 2.2423332e-14 3.4188208e-12
  1.2747656e-18 3.3658313e-26 3.9173267e-24 0.0000000e+00 0.0000000e+00
  8.1268878e-13 2.0598579e-16 3.3637093e-36 1.3548737e-21 1.9083636e-19
  0.0000000e+00 0.0000000e+00 2.2670825e-14]]
1/1 [==============================] - 0s 27ms/step
1/1 [==============================] - 0s 17ms/step
Predicted class index of coefficient: 6 
The label of class index of coefficient: 1 
Predicted class index of variable: 6 
The label of class index of variable: mu4

------

The final answer: 1mu4
 
